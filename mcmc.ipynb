{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "%matplotlib inline\n",
    "import emcee\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from spectra import Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, sqrt, pi\n",
    "def lnL(theta, x, y, yerr):\n",
    "    #a,b=theta\n",
    "    amp,sigma,lambda_rest = theta\n",
    "    #model = b * x + a\n",
    "    model = (amp / (sqrt(2*pi) * sigma)) * exp(-(x-lambda_rest)**2 / (2*sigma**2))\n",
    "    inv_sigma2 = 1.0/(yerr**2)\n",
    "    return -0.5*(np.sum((y-model)**2*inv_sigma2))\n",
    "\n",
    "def lnprior(theta):\n",
    "    amp,sigma,lambda_rest = theta\n",
    "    #amp,deltaLambda,sigma = theta\n",
    "    if 0 < sigma < 10 and 4000 < lambda_rest < 7000:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def lnprob(theta, x, y, yerr):\n",
    "    \"\"\"\n",
    "    The likelihood to include in the MCMC.\n",
    "    \"\"\"\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnL(theta, x, y, yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem3\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import emcee\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "\n",
    "\n",
    "def run_emcee(x, y, y_obs, sigma, ml_result): #ml_result is the result object from lmfit\n",
    "\n",
    "    # Set up the properties of the problem.\n",
    "    ndim, nwalkers = 3, 200\n",
    "\n",
    "    # Setup a bunch of starting positions.\n",
    "    pos = [ml_result + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "        \n",
    "    # Create the sampler.\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x, y_obs, sigma))\n",
    "\n",
    "    sampler.run_mcmc(pos, 500)\n",
    "\n",
    "    samples = sampler.chain[:, 50:, :].reshape((-1, 3))\n",
    "\n",
    "    return sampler, samples\n",
    "\n",
    "\n",
    "\n",
    "def show_walkers(chain, labels, savefile=None):\n",
    "\n",
    "    nwalkers, nreps, ndim = chain.shape\n",
    "\n",
    "    xval = np.arange(0, nreps)\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=1, nrows=ndim, figsize=(14, 4))\n",
    "    for i_dim in range(ndim):\n",
    "        ax[i_dim].set_ylabel(labels[i_dim])\n",
    "        \n",
    "        for i in range(100):\n",
    "            ax[i_dim].plot(xval, chain[i, :, i_dim], color='black', alpha=0.5)\n",
    "\n",
    "    if savefile is not None:\n",
    "        plt.savefig(savefile)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def show_corner_plot(samples,labels,truths, savefile=None):\n",
    "\n",
    "    fig = corner.corner(samples, labels=labels, truths=truths, quantiles=[0.16,0.5,0.84])\n",
    "\n",
    "    \n",
    "    if savefile is not None:\n",
    "        plt.savefig(savefile)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Spectrum('src/data-spectrum/new/idid000007077jd2458163p6789f000.fits')\n",
    "#spec.plot('-r',4340,4780)\n",
    "#lambda0 = 6562.85175\n",
    "lambda0 = 4861.297\n",
    "#spec.gfitting([lT],False)\n",
    "#spec.plot('-r',4840,4880)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(dlambda)\n",
    "#print(sigma)\n",
    "data = spec.fit_unique(lambda0,4840,4880)\n",
    "amp = data['result'].params['amp'].value\n",
    "sigma = data['result'].params['sigma'].value\n",
    "lambda_rest = data['result'].params['lambda0'].value\n",
    "#print(amp,sigma,lambda_rest)\n",
    "#data = np.random.normal(loc=100.0, scale=3.0, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See ml_illustration for the run_ml() function that calculates these numbers.\n",
    "p_initial = [amp,sigma,lambda_rest]#values from result object\n",
    "\n",
    "sampler_gaussian, samples_gaussian = run_emcee(data['x'], data['y'], data['y'], data['sigma'], p_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = show_walkers(sampler_gaussian.chain, ['amp', 'sigma', 'lambda_rest'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a corner plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = show_corner_plot(samples_gaussian, ['amp','sigma','lambda_rest'], p_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burning in ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Probability function returned NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-34389eaeb18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Run 200 steps as a burn-in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Burning in ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_mcmc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Reset the chain to remove the burn-in samples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/emcee/ensemble.py\u001b[0m in \u001b[0;36mrun_mcmc\u001b[0;34m(self, initial_state, nsteps, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/emcee/ensemble.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, initial_state, log_prob0, rstate0, blobs0, iterations, tune, skip_initial_state_check, thin_by, thin, store, progress)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0;31m# Propose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/emcee/moves/red_blue.py\u001b[0m in \u001b[0;36mpropose\u001b[0;34m(self, model, state)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Compute the lnprobs of the proposed position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mnew_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_log_prob_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Loop over the walkers and update them accordingly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/emcee/ensemble.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, coords)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# Check for log_prob returning NaN.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Probability function returned NaN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Probability function returned NaN"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MCMC fitting template. \n",
    "This template fits a 1-d gaussian, if you \n",
    "figure out how to use it for more complicated distributions\n",
    "I'd appreciate if you let me know :)\n",
    "banados@mpia.de\n",
    "'''\n",
    "\n",
    "#First let's create a gaussian data\n",
    "\n",
    "data = np.random.normal(loc=100.0, scale=3.0, size=1000)\n",
    "\n",
    "# Then, define the probability distribution that you would like to sample.\n",
    "def lnprob(p, vec):\n",
    "    diff = vec-p[0]\n",
    "    N = len(vec)\n",
    "    return -0.5 * N * np.log(2 * np.pi) - N * np.log(p[1]) - 0.5 \\\n",
    "                                    * np.sum(( (vec - p[0]) / p[1] ) ** 2)\n",
    "    \n",
    "               \n",
    "# We'll sample a Gaussian which has 2 parameters: mean and sigma...\n",
    "ndim = 2\n",
    "\n",
    "# We'll sample with 250 walkers. (nwalkers must be an even number)\n",
    "nwalkers = 250\n",
    "\n",
    "# Choose an initial set of positions for the walkers.\n",
    "p0 = [np.random.rand(ndim) for i in range(nwalkers)]\n",
    "\n",
    "# Initialize the sampler with the chosen specs.\n",
    "#The \"a\" parameter controls the step size, the default is a=2,\n",
    "#but in this case works better with a=4 see below or page 10 in the paper\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=[data], a=4)\n",
    "\n",
    "# Run 200 steps as a burn-in.\n",
    "print(\"Burning in ...\")\n",
    "pos, prob, state = sampler.run_mcmc(p0, 200)\n",
    "\n",
    "# Reset the chain to remove the burn-in samples.\n",
    "sampler.reset()\n",
    "\n",
    "# Starting from the final position in the burn-in chain, sample for 1000\n",
    "# steps. (rstate0 is the state of the internal random number generator)\n",
    "print(\"Running MCMC ...\")\n",
    "pos, prob, state = sampler.run_mcmc(pos, 1000, rstate0=state)\n",
    "\n",
    "# Print out the mean acceptance fraction. In general, acceptance_fraction\n",
    "# has an entry for each walker so, in this case, it is a 250-dimensional\n",
    "# vector.\n",
    "af = sampler.acceptance_fraction\n",
    "print(\"Mean acceptance fraction:\", np.mean(af))\n",
    "af_msg = '''As a rule of thumb, the acceptance fraction (af) should be \n",
    "                            between 0.2 and 0.5\n",
    "            If af < 0.2 decrease the a parameter\n",
    "            If af > 0.5 increase the a parameter\n",
    "            '''\n",
    "\n",
    "print(af_msg)\n",
    "\n",
    "# If you have installed acor (http://github.com/dfm/acor), you can estimate\n",
    "# the autocorrelation time for the chain. The autocorrelation time is also\n",
    "# a vector with 10 entries (one for each dimension of parameter space).\n",
    "try:\n",
    "    print(\"Autocorrelation time:\", sampler.acor)\n",
    "except ImportError:\n",
    "    print(\"You can install acor: http://github.com/dfm/acor\")\n",
    "    \n",
    "maxprob_indice = np.argmax(prob)\n",
    "\n",
    "mean_fit, sigma_fit = pos[maxprob_indice]\n",
    "\n",
    "print(\"Estimated parameters: mean, sigma = %f, %f\" % (mean_fit, sigma_fit))\n",
    "\n",
    "mean_samples = sampler.flatchain[:,0]\n",
    "sigma_samples = sampler.flatchain[:,1]\n",
    "\n",
    "mean_std = mean_samples.std()\n",
    "sigma_std = sigma_samples.std()\n",
    "\n",
    "print(\"parameters' error: mean, sigma = %f, %f\" % (mean_std, sigma_std))\n",
    "\n",
    "# Finally, you can plot the projected histograms of the samples using\n",
    "# matplotlib as follows (as long as you have it installed).\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    print(\"Try installing matplotlib to generate some sweet plots...\")\n",
    "else:\n",
    "    plt.hist(mean_samples, 100)\n",
    "    plt.title(\"Samples for mean\")\n",
    "    plt.show()\n",
    "    plt.title(\"Samples for sigma\")\n",
    "    plt.hist(sigma_samples, 100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
